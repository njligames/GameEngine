// This file was created by Filewrap 1.2
// Little endian mode
// DO NOT EDIT

#include "../PVRTMemoryFileSystem.h"

// using 32 bit to guarantee alignment.
#ifndef A32BIT
 #define A32BIT static const unsigned int
#endif

// ******** Start: ComputeShader.csh ********

// File data
static const char _ComputeShader_csh[] = 
	"#version 310 es\r\n"
	"\r\n"
	"/*********** INPUT DEFINES: THESE MUST BE SELECTED AND DEFINED BY THE PROGRAM ***********/\r\n"
	"/*\r\n"
	"IMAGE_BINDING_INPUT  : Image unit used for the texture that contains the current generation\r\n"
	"IMAGE_BINDING_OUTPUT : Image unit used for the texture that contains the current generation\r\n"
	"WG_WIDTH\t\t\t : Dimension X of the workgroup size\r\n"
	"WG_HEIGHT\t\t\t : Dimension Y of the workgroup size\r\n"
	"*/\r\n"
	"/*********************************** END INPUT DEFINES **********************************/\r\n"
	"\r\n"
	"//These defines determine local memory requirement (\"cache\" in this shader) depending on Workgroup size\r\n"
	"#define CACHE_WIDTH (WG_WIDTH + 2)\r\n"
	"#define CACHE_HEIGHT (WG_HEIGHT + 2)\r\n"
	"#define CACHE_AREA (CACHE_WIDTH * CACHE_HEIGHT)\r\n"
	"\r\n"
	"uniform layout(rgba8, binding = IMAGE_BINDING_INPUT) readonly highp image2D imageIn;\r\n"
	"uniform layout(rgba8, binding = IMAGE_BINDING_OUTPUT) writeonly highp image2D imageOut;\r\n"
	"shared float local_cache[CACHE_AREA+1]; //+1 so that we don't have to guard for a single out-of-bounds\r\n"
	"\r\n"
	"/*\r\n"
	" *   -------------------------------------------- CACHING STRATEGY --------------------------------------------\r\n"
	" *   GLSL Compute shaders, DirectX compute shaders and OpenCL all define the concept of \"shared\" or \"local\" memory,\r\n"
	" *   which instructed to be fast, on-chip memory local to a workgroup and shared by all threads (work items et.c.,\r\n"
	" *   depending on terminology). \r\n"
	" *   Much of the optimization of GPGPU programs that have a concept of reading data \"around\" their position in an\r\n"
	" *   input domain is finding the correct strategy to utilize local memory.\r\n"
	" *   In this case, each shader will need to read the input texture at own cell location and all neighbouring cells.\r\n"
	" *   That means that apart from the area of the grid that corresponds to our workgroup, we also need the border of that area.\r\n"
	" * \r\n"
	" *   It is obvious that it is impossible to have all threads doing 100% the same amount of work, as our total domain\r\n"
	" *   (i.e. the part of the grid that is our workgroup plus its borders) is greater than AND not a multiple of our \r\n"
	" *   workgroup size.\r\n"
	" *\r\n"
	" *   By examining the problem parameters (leaving the math as an exercise to the reader) we also see that, unless our \r\n"
	" *   workgroup is less than 8x4 (Rogue warp size is 32, hence this would be the minimum as well!), the domain is \r\n"
	" *   always less than twice the size of our workgroup.\r\n"
	" *   \r\n"
	" *   In that case, we will exploit that information to fetch either 1 or 2 texels per thread, by mapping our 2D local \r\n"
	" *   coordinates to a 1D cache area (cache[]), and use each thread to fetch 2 texels, skipping the ones that are not needed.\r\n"
	" *   In order for that to work, each item does NOT fetch its own data from global memory (the texture), but the ones that \r\n"
	" *   the mapping dictates by sequentially getting two items per thread, until we cover the entire sampling area.\r\n"
	" *   \r\n"
	" *   Workgroup area : height x width\r\n"
	" *   Cache area     : (height+2) x (width+2).\r\n"
	" *   Cache index 1 = local index y * group width + local index x\r\n"
	" *   Cache index 2 = Cache index 1 + 1\r\n"
	" *   Et cetera...\r\n"
	" *   Using the algorithm below, the items will be fetched in the following order (example :4x8 kernel => 6x10 cache) \r\n"
	" *   (0,0), (0,0), (0,1), (0,1), (0,2), (0,2), \r\n"
	" *   (0,3), (0,3), (1,0), (1,0), (1,1), (1,1), \r\n"
	" *   (1,2), (1,2), (1,3), (1,3), (2,0), (2,0),\r\n"
	" *   (2,1), (2,1), (2,2), (2,2), (2,3), (2,3),\r\n"
	" *   (3,0), (3,0), (3,1), (3,1), (3,2), (3,2),\r\n"
	" *   (3,3), (3,3), (4,0), (4,0), (4,1), (4,1),\r\n"
	" *   (4,2), (4,2), (4,3), (4,3), (5,0), (5,0),\r\n"
	" *   (5,1), (5,1), (5,2), (5,2), (5,3), (5,3),\r\n"
	" *   (6,0), (6,0), (6,1), (6,1), (6,2), (6,2),\r\n"
	" *   (6,3), (6,3), (7,0), (7,0), (7,1), (7,1) DONE\r\n"
	" *   THE FOLLOWING THREADS ARE SKIPPED BY THE CACHE INDEX CHECK:\r\n"
	" *   (7,2), (7,2), (7,3), (7,3)\r\n"
	" *\r\n"
	" *   NOTE : This strategy spreads the texture fetching optimally among threads, but introduces a few extra ALU ops. Depending \r\n"
	" *   on a many factors, in case for example ALU instructions were a bottleneck, another strategy might be more suitable, for\r\n"
	" *   example making indexing simpler with fewer ops but paying the cost of less well distributed texture fetches.\r\n"
	" */\r\n"
	" \r\n"
	" /* ************************************************************************************************************************\r\n"
	"  * @Function prefetch_texture_samples\r\n"
	"  * @Input grid   : The global 2D coordinates of the top-left (0,0) element of our workgroup (used to offset into the image)\r\n"
	"  * @Input lid    : The local 2D coordinates of our work-item\r\n"
	"  * @Input bounds : The maximum size of the image\r\n"
	"  * Will fetch a sample into the shared workgroup cache, based on the local id of the item. \r\n"
	"  * ************************************************************************************************************************/\r\n"
	"void prefetch_texture_samples(ivec2 grid, ivec2 lid, int cache_id, ivec2 bounds)\r\n"
	"{\r\n"
	"\tif (cache_id < CACHE_AREA)\r\n"
	"\t{\r\n"
	"\t\t//Map the cache index to the actual global coordinates that must be fetched\r\n"
	"\t\tconst ivec2 c_01 = ivec2(0,1);\r\n"
	"\t\tivec2 cache_ids = ivec2(cache_id, cache_id) + ivec2(0,1);\r\n"
	"\r\n"
	"\t\tivec2 idByCacheWidth = cache_ids / CACHE_WIDTH;\r\n"
	"\r\n"
	"\t\tivec4 offset = ivec4(cache_ids - idByCacheWidth * CACHE_WIDTH - 1, idByCacheWidth - 1);\r\n"
	"\r\n"
	"\t\tivec4 coords = grid.xxyy + offset;\r\n"
	"\t\tlocal_cache[cache_ids.x] = imageLoad(imageIn, coords.xz).x;\r\n"
	"\t\tlocal_cache[cache_ids.y] = imageLoad(imageIn, coords.yw).x;\r\n"
	"\t}\r\n"
	"}\r\n"
	"\r\n"
	"\r\n"
	"//In general, if we can use #defines that are passed to the shader at compile time we can achieve far better \r\n"
	"//flexibility to optimize for a specific platform\r\n"
	"layout (local_size_x = WG_WIDTH, local_size_y = WG_HEIGHT) in;\r\n"
	"void main()\r\n"
	"{\r\n"
	"\tivec2 sz = imageSize(imageIn);\r\n"
	"\tivec2 gid = ivec2(gl_GlobalInvocationID.xy);\r\n"
	"\tivec2 lid = ivec2(gl_LocalInvocationID.xy);\r\n"
	"\tivec2 grid = ivec2(gl_WorkGroupID.xy * gl_WorkGroupSize.xy); //Global coords of the top left corner of our WG in order to offset to it\r\n"
	"\t\r\n"
	"\tint baseid = (lid.y * WG_WIDTH + lid.x) * 2;\r\n"
	"\t\r\n"
	"\t//Get items from global memory.\r\n"
	"\tprefetch_texture_samples(grid, lid, baseid, sz);\r\n"
	"\t//No need to sync yet - no two threads they do not touch the same values.\r\n"
	"\t//prefetch_texture_sample(grid, lid, baseid+1, sz);\r\n"
	"\r\n"
	"\t//CAUTION --- We need explicit, full execution synchronization here  - barrier() - in order to ensure that all threads have actually reached\r\n"
	"\t//this execution point. A simple memoryBarrier...() would not guarrantee that all threads in the group have actually executed - only that \r\n"
	"\t//what HAS executed is visible to the rest of the workgroup.\r\n"
	"\tbarrier();\r\n"
	"\r\n"
	"\t//CAUTION --- Bounds checking (for domain that is not divisible by our WG size) must also be done AFTER fetching for the cache : with our \r\n"
	"\t//prefetching algorithm, inactive threads may still be responsible for fetching data.\r\n"
	"\tif (gid.x >= sz.x || gid.y >= sz.y) { return; }\r\n"
	"\t\r\n"
	"\t//Sum up the amount of neighbours for the thread. Everything is now loaded into shared memory.\r\n"
	"\t//For our cached area, Index 0 is the item that in local coords would be (-1,-1) (that is, the item upwards and leftwards of our first, (0,0) item)\r\n"
	"\t//and the rest follows suite keeping in mind that the local cache's width also has the borders (workgroup width + 2). \r\n"
	"\t//We are vectorising and reusing ALU ops as much as possible.\r\n"
	"\t\r\n"
	"\tconst ivec3 c_012 = ivec3(0,1,2);\r\n"
	"\tivec3 idx_y = (lid.yyy + c_012) * ivec3(CACHE_WIDTH,CACHE_WIDTH,CACHE_WIDTH);\r\n"
	"\tivec3 idx_x = lid.xxx + c_012;\r\n"
	"\r\n"
	"\tfloat nNeighbours\t= local_cache[idx_y.x + idx_x.x]\t+\tlocal_cache[idx_y.x + idx_x.y]\t+ local_cache[idx_y.x + idx_x.z]\r\n"
	"\t\t\t\t\t\t+ local_cache[idx_y.y + idx_x.x]          /* omit myself */\t\t\t\t+ local_cache[idx_y.y + idx_x.z]\r\n"
	"\t\t\t\t\t\t+ local_cache[idx_y.z + idx_x.x]\t+\tlocal_cache[idx_y.z + idx_x.y]\t+ local_cache[idx_y.z + idx_x.z];\r\n"
	"\tfloat self = local_cache[idx_y.y + idx_x.y];\r\n"
	"\r\n"
	"\t//*********************** The deceptively simple Conway's Game of Life algorithm ***********************//\r\n"
	"\t//ALGORITHM (UNOPTIMISED) CODE\r\n"
	"\t//For an empty cell...\r\n"
	"\t//float nextState = 0.0;\r\n"
	"\t//if (self < 1.)\t\r\n"
	"\t//{\r\n"
	"\t//\t//... be born if there are exactly three neighbours.\r\n"
	"\t//\tif (nNeighbours == 3.0f) { nextState = 1.; }\r\n"
	"\t//}\r\n"
	"\t////... while, for a live cell...\r\n"
	"\t//else\r\n"
	"\t//{\r\n"
	"\t//\t//... die, from loneliness or overcrowding if there are less than two or more than three neighbours\r\n"
	"\t//\tif((nNeighbours < 2.0f) || (nNeighbours > 3.0f))\r\n"
	"\t//\t\tnextState = 0.;\r\n"
	"\t//\t//... otherwise, stay alive\r\n"
	"\t//\telse\r\n"
	"\t//\t\tnextState = 1.;\t\t\t\r\n"
	"\t//}\r\n"
	"\r\n"
	"\t//OPTIMISED Expression : the slightly faster following expression (eliminate 3 branches in favor of a \r\n"
	"\t//mix and two convert to float).\r\n"
	"\tfloat nextState = mix(float(nNeighbours == 3.), float(nNeighbours >= 2.0f && nNeighbours <= 3.0f), self);\r\n"
	"\t//This works thus : \"Mix\" can be used as a mask with values that are either bool, or 0/1 by choosing between them.\r\n"
	"\t//Hence, the \"mix\" instruction is the outer if.\r\n"
	"\t//The inner instructions could not be optimised further, but boolean converts to float 0/1 which suits us just fine.\r\n"
	"\r\n"
	"\t//Store red into the output image.\r\n"
	"\timageStore(imageOut, gid, vec4(nextState,0.,0.,1.));\r\n"
	"\r\n"
	"\t//CAUTION : Image Store is *INCOHERENT* (see opengl memory model) - it will need to be explicitly synchronized \r\n"
	"\t//through the OpenGL API with glMemoryBarrier(...) with the appropriate barrier bits set before the written \r\n"
	"\t//memory is used in another shader or other use. For this example (in which it must be fed back as Image variable,\r\n"
	"\t//GL_SHADER_IMAGE_ACCESS_BARRIER_BIT will need to be used.\r\n"
	"}\r\n";

// Register ComputeShader.csh in memory file system at application startup time
static CPVRTMemoryFileSystem RegisterFile_ComputeShader_csh("ComputeShader.csh", _ComputeShader_csh, 9480);

// ******** End: ComputeShader.csh ********

